{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "32yCsRUo8H33"
      },
      "source": [
        "# 2024 COMP90042 Project\n",
        "*Make sure you change the file name with your group id.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XCybYoGz8YWQ"
      },
      "source": [
        "# Readme\n",
        "*If there is something to be noted for the marker, please mention here.*\n",
        "\n",
        "*If you are planning to implement a program with Object Oriented Programming style, please put those the bottom of this ipynb file*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6po98qVA8bJD"
      },
      "source": [
        "# 1.DataSet Processing\n",
        "(You can add as many code blocks and text blocks as you need. However, YOU SHOULD NOT MODIFY the section title)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {},
      "outputs": [],
      "source": [
        "import re\n",
        "import nltk\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import json\n",
        "import ijson"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to\n",
            "[nltk_data]     C:\\Users\\Kaiya\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     C:\\Users\\Kaiya\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "#Pre-processing \n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "nltk.download('wordnet')\n",
        "lemmatizer = nltk.stem.wordnet.WordNetLemmatizer()\n",
        "nltk.download('stopwords')\n",
        "stopwords = set(stopwords.words('english'))\n",
        "\n",
        "def lemmatize(word):\n",
        "    lemma = lemmatizer.lemmatize(word, 'v')\n",
        "    if lemma == word:\n",
        "        lemma = lemmatizer.lemmatize(word, 'n')\n",
        "    return lemma\n",
        "\n",
        "def preprocess_text(text):\n",
        "    if text:\n",
        "        text = text.lower()\n",
        "        text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n",
        "        text = re.sub(r'\\s+', ' ', text).strip()\n",
        "        \n",
        "        words = text.split()\n",
        "        new_words = []\n",
        "        for w in words:\n",
        "            w = lemmatize(w)\n",
        "            if w not in stopwords:\n",
        "                new_words.append(w)\n",
        "        text = \" \".join(new_words)\n",
        "    return text\n",
        "\n",
        "def text_preprocessing(data_set):\n",
        "    for item in data_set:\n",
        "        item['claim_text'] = preprocess_text(item['claim_text'])\n",
        "        item['evidence_texts'] = [preprocess_text(evidence) for evidence in item['evidence_texts']]\n",
        "    return data_set\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load all file with json\n",
        "train_claims = json.load(open(\"C:/Users/Kaiya/Desktop/COMP90042_2024-main/data/train-claims.json\", \"r\", encoding=\"utf-8\"))\n",
        "dev_claims = json.load(open(\"C:/Users/Kaiya/Desktop/COMP90042_2024-main/data/dev-claims.json\", \"r\", encoding=\"utf-8\"))\n",
        "test_claims = json.load(open(\"C:/Users/Kaiya/Desktop/COMP90042_2024-main/data/test-claims-unlabelled.json\", \"r\", encoding=\"utf-8\"))\n",
        "evidences = json.load(open(\"C:/Users/Kaiya/Desktop/COMP90042_2024-main/data/evidence.json\", \"r\", encoding=\"utf-8\"))\n",
        "\n",
        "#Main evidence file. Creating empty lists for each text, ID, evidence, label\n",
        "evidences_texts = []\n",
        "evidences_ids = []\n",
        "evidences_id_dict = {}\n",
        "idx = 0\n",
        "#For each evidence, seperate it's id, text etc and added into the empty list\n",
        "for evidence_id, evidence_text in evidences.items():\n",
        "    evidences_ids.append(evidence_id)\n",
        "    evidences_texts.append(text_preprocessing(evidence_text))\n",
        "    evidences_id_dict[evidence_id] = idx\n",
        "    idx += 1\n",
        "\n",
        "\n",
        "train_texts = []\n",
        "train_evidences = []\n",
        "train_labels = []\n",
        "train_ids = []\n",
        "for train_id, data in train_claims.items():\n",
        "    train_ids.append(train_id)\n",
        "    train_texts.append(text_preprocessing(data[\"claim_text\"]))\n",
        "    train_labels.append(data[\"claim_label\"])\n",
        "    train_evidences.append([evidences_id_dict[i] for i in data[\"evidences\"]])\n",
        "\n",
        "\n",
        "dev_texts = []\n",
        "dev_evidences = []\n",
        "dev_labels = []\n",
        "dev_ids = []\n",
        "for dev_id, data in dev_claims.items():\n",
        "    dev_ids.append(dev_id)\n",
        "    dev_texts.append(text_preprocessing(data[\"claim_text\"]))\n",
        "    dev_labels.append(data[\"claim_label\"])\n",
        "    dev_evidences.append([evidences_id_dict[i] for i in data[\"evidences\"]])\n",
        "\n",
        "\n",
        "test_ids = []\n",
        "test_texts = []\n",
        "for test_id, data in test_claims.items():\n",
        "    test_ids.append(test_id)\n",
        "    test_texts.append(text_preprocessing(data[\"claim_text\"]))\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {},
      "outputs": [],
      "source": [
        "# create temperary file stroe location: mkdir temp_data (in terminal)\n",
        "json.dump(train_ids, open(\"C:/Users/Kaiya/Desktop/COMP90042_2024-main/temp_data/train_ids.json\", \"w\"))\n",
        "json.dump(train_texts, open(\"C:/Users/Kaiya/Desktop/COMP90042_2024-main/temp_data/train_texts.json\", \"w\"))\n",
        "json.dump(train_evidences, open(\"C:/Users/Kaiya/Desktop/COMP90042_2024-main/temp_data/train_evidences.json\", \"w\"))\n",
        "json.dump(train_labels, open(\"C:/Users/Kaiya/Desktop/COMP90042_2024-main/temp_data/train_labels.json\", \"w\"))\n",
        "\n",
        "json.dump(dev_ids, open(\"C:/Users/Kaiya/Desktop/COMP90042_2024-main/temp_data/dev_ids.json\", \"w\"))\n",
        "json.dump(dev_texts, open(\"C:/Users/Kaiya/Desktop/COMP90042_2024-main/temp_data/dev_texts.json\", \"w\"))\n",
        "json.dump(dev_evidences, open(\"C:/Users/Kaiya/Desktop/COMP90042_2024-main/temp_data/dev_evidences.json\", \"w\"))\n",
        "json.dump(dev_labels, open(\"C:/Users/Kaiya/Desktop/COMP90042_2024-main/temp_data/dev_labels.json\", \"w\"))\n",
        "\n",
        "json.dump(test_ids, open(\"C:/Users/Kaiya/Desktop/COMP90042_2024-main/temp_data/test_ids.json\", \"w\"))\n",
        "json.dump(test_texts, open(\"C:/Users/Kaiya/Desktop/COMP90042_2024-main/temp_data/test_texts.json\", \"w\"))\n",
        "json.dump(evidences_texts, open(\"C:/Users/Kaiya/Desktop/COMP90042_2024-main/temp_data/evidences_texts.json\", \"w\"))\n",
        "json.dump(evidences_ids, open(\"C:/Users/Kaiya/Desktop/COMP90042_2024-main/temp_data/evidences_ids.json\", \"w\"))\n",
        "json.dump(evidences_id_dict, open(\"C:/Users/Kaiya/Desktop/COMP90042_2024-main/temp_data/evidences_id_dict.json\", \"w\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {},
      "outputs": [],
      "source": [
        "# make the files easier to access after debugging\n",
        "train_claims = json.load(open(\"C:/Users/Kaiya/Desktop/COMP90042_2024-main/data/train-claims.json\", \"r\", encoding=\"utf-8\"))\n",
        "dev_claims = json.load(open(\"C:/Users/Kaiya/Desktop/COMP90042_2024-main/data/dev-claims.json\", \"r\", encoding=\"utf-8\"))\n",
        "test_claims = json.load(open(\"C:/Users/Kaiya/Desktop/COMP90042_2024-main/data/test-claims-unlabelled.json\", \"r\", encoding=\"utf-8\"))\n",
        "\n",
        "train_ids = json.load(open(\"C:/Users/Kaiya/Desktop/COMP90042_2024-main/temp_data/train_ids.json\", \"r\", encoding=\"utf-8\"))\n",
        "train_texts = json.load(open(\"C:/Users/Kaiya/Desktop/COMP90042_2024-main/temp_data/train_texts.json\", \"r\", encoding=\"utf-8\"))\n",
        "train_evidences = json.load(open(\"C:/Users/Kaiya/Desktop/COMP90042_2024-main/temp_data/train_evidences.json\", \"r\", encoding=\"utf-8\"))\n",
        "train_labels = json.load(open(\"C:/Users/Kaiya/Desktop/COMP90042_2024-main/temp_data/train_labels.json\", \"r\", encoding=\"utf-8\"))\n",
        "\n",
        "dev_ids = json.load(open(\"C:/Users/Kaiya/Desktop/COMP90042_2024-main/temp_data/dev_ids.json\", \"r\", encoding=\"utf-8\"))\n",
        "dev_texts = json.load(open(\"C:/Users/Kaiya/Desktop/COMP90042_2024-main/temp_data/dev_texts.json\", \"r\", encoding=\"utf-8\"))\n",
        "dev_evidences = json.load(open(\"C:/Users/Kaiya/Desktop/COMP90042_2024-main/temp_data/dev_evidences.json\", \"r\", encoding=\"utf-8\"))\n",
        "dev_labels = json.load(open(\"C:/Users/Kaiya/Desktop/COMP90042_2024-main/temp_data/dev_labels.json\", \"r\", encoding=\"utf-8\"))\n",
        "\n",
        "test_ids = json.load(open(\"C:/Users/Kaiya/Desktop/COMP90042_2024-main/temp_data/test_ids.json\", \"r\", encoding=\"utf-8\"))\n",
        "test_texts = json.load(open(\"C:/Users/Kaiya/Desktop/COMP90042_2024-main/temp_data/test_texts.json\", \"r\", encoding=\"utf-8\"))\n",
        "evidences_texts = json.load(open(\"C:/Users/Kaiya/Desktop/COMP90042_2024-main/temp_data/evidences_texts.json\", \"r\", encoding=\"utf-8\"))\n",
        "evidences_ids = json.load(open(\"C:/Users/Kaiya/Desktop/COMP90042_2024-main/temp_data/evidences_ids.json\", \"r\", encoding=\"utf-8\"))\n",
        "evidences_id_dict = json.load(open(\"C:/Users/Kaiya/Desktop/COMP90042_2024-main/temp_data/evidences_id_dict.json\", \"r\", encoding=\"utf-8\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Tfi-Df from Sklearn\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# added how much words into corpus (50000) \n",
        "vectorizer = TfidfVectorizer(max_features=50000 , min_df= 1)\n",
        "vectorizer.fit(evidences_texts + train_texts + test_texts)\n",
        "\n",
        "train_tfidf = vectorizer.transform(train_texts)\n",
        "dev_tfidf = vectorizer.transform(dev_texts)\n",
        "test_tfidf = vectorizer.transform(test_texts)\n",
        "evidence_tfidf = vectorizer.transform(evidences_texts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(1228, 50000)\n",
            "(1208827, 50000)\n"
          ]
        }
      ],
      "source": [
        "print(train_tfidf.shape)\n",
        "print(evidence_tfidf.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {},
      "outputs": [],
      "source": [
        "# using numpy and scikit to calcualte the cosine similarity\n",
        "train_cos_sims = np.dot(train_tfidf, evidence_tfidf.transpose()).toarray()\n",
        "dev_cos_sims = np.dot(dev_tfidf, evidence_tfidf.transpose()).toarray()\n",
        "test_cos_sims = np.dot(test_tfidf, evidence_tfidf.transpose()).toarray()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1FA2ao2l8hOg"
      },
      "source": [
        "# 2. Model Implementation\n",
        "(You can add as many code blocks and text blocks as you need. However, YOU SHOULD NOT MODIFY the section title)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QIEqDDT78q39"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EzGuzHPE87Ya"
      },
      "source": [
        "# 3.Testing and Evaluation\n",
        "(You can add as many code blocks and text blocks as you need. However, YOU SHOULD NOT MODIFY the section title)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6ZVeNYIH9IaL"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mefSOe8eTmGP"
      },
      "source": [
        "## Object Oriented Programming codes here\n",
        "\n",
        "*You can use multiple code snippets. Just add more if needed*"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3.8.6 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    },
    "vscode": {
      "interpreter": {
        "hash": "e0144baad0ecee903f108a3e46e51ceadd7da3fc904cfa79747d813b61464b4e"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
